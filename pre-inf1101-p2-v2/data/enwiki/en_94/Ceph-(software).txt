Ceph (pronounced ) is an open-source software storage platform, implements object storage on a single distributed computer cluster, and provides 3in1 interfaces for : object-, block- and file-level storage. Ceph aims primarily for completely distributed operation without a single point of failure, scalable to the exabyte level, and freely available. Ceph replicates data and makes it fault-tolerant, using commodity hardware and requiring no specific hardware support. As a result of its design, the system is both self-healing and self-managing, aiming to minimize administration time and other costs. ==Design== A high-level overview of the Ceph's internal organization Ceph employs five distinct kinds of daemons: * Cluster monitors () that keep track of active and failed cluster nodes, cluster configuration, and information about data placement and global cluster state. * Object storage devices () that use a direct, journaled disk storage (named BlueStore, which since the v12.x release replaces the FileStore which would use a filesystem) * Metadata servers () that cache and broker access to inodes and directories inside a CephFS filesystem. * HTTP gateways () that expose the object storage layer as an interface compatible with Amazon S3 or OpenStack Swift APIs * Managers () that perform cluster monitoring, bookkeeping, and maintenance tasks, and interface to external monitoring systems and management (e.g. balancer, dashboard, Prometheus, Zabbix plugin) All of these are fully distributed, and may run on the same set of servers. Clients with different needs can directly interact with different subsets of them. Ceph does striping of individual files across multiple nodes to achieve higher throughput, similar to how RAID0 stripes partitions across multiple hard drives. Adaptive load balancing is supported whereby frequently accessed objects are replicated over more nodes. , BlueStore is the default and recommended storage type for production environments, which is Ceph's own storage implementation providing better latency and configurability than the filestore backend, and avoiding the shortcomings of the filesystem based storage involving additional processing and caching layers. The Filestore backend is still considered useful and very stable; XFS used to be the recommended underlying filesystem type for production environments, while Btrfs was recommended for non-production environments. ext4 filesystems were not recommended because of resulting limitations on the maximum RADOS objects length. Even using BlueStore, XFS is used for a small partition of metadata. ===Object storage=== An architecture diagram showing the relations between components of the Ceph storage platform Ceph implements distributed object storage - BlueStore. Ceph's software libraries provide client applications with direct access to the reliable autonomic distributed object store (RADOS) object-based storage system, and also provide a foundation for some of Ceph's features, including RADOS Block Device (RBD), RADOS Gateway, and the Ceph File System. The "librados" software libraries provide access in C, C++, Java, PHP, and Python. The RADOS Gateway also exposes the object store as a RESTful interface which can present as both native Amazon S3 and OpenStack Swift APIs. ===Block storage=== Ceph's object storage system allows users to mount Ceph as a thin-provisioned block device. When an application writes data to Ceph using a block device, Ceph automatically stripes and replicates the data across the cluster. Ceph's RADOS Block Device (RBD) also integrates with Kernel-based Virtual Machines (KVMs). Ceph RBD interfaces with the same Ceph object storage system that provides the librados interface and the CephFS file system, and it stores block device images as objects. Since RBD is built on librados, RBD inherits librados's abilities, including read-only snapshots and revert to snapshot. By striping images across the cluster, Ceph improves read access performance for large block device images. The block device can be virtualized, providing block storage to virtual machines, in virtualization platforms such as Apache CloudStack, OpenStack, OpenNebula, Ganeti, and Proxmox Virtual Environment. ===File system=== Ceph's file system (CephFS) runs on top of the same object storage system that provides object storage and block device interfaces. The Ceph metadata server cluster provides a service that maps the directories and file names of the file system to objects stored within RADOS clusters. The metadata server cluster can expand or contract, and it can rebalance the file system dynamically to distribute data evenly among cluster hosts. This ensures high performance and prevents heavy loads on specific hosts within the cluster. Clients mount the POSIX-compatible file system using a Linux kernel client. An older FUSE-based client is also available. The servers run as regular Unix daemons. ==History== Ceph was initially created by Sage Weil for his doctoral dissertation, which was advised by Professor Scott A. Brandt at the Jack Baskin School of Engineering, University of California, Santa Cruz (UCSC), and sponsored by the Advanced Simulation and Computing Program (ASC), including Los Alamos National Laboratory (LANL), Sandia National Laboratories (SNL), and Lawrence Livermore National Laboratory (LLNL). The first line of code that ended up being part of Ceph was written by Sage Weil in 2004 while at a summer internship at LLNL, working on scalable filesystem metadata management (known today as Ceph's MDS).Dynamic Metadata Management for Petabyte-Scale File Systems, SA Weil, KT Pollack, SA Brandt, EL Miller, Proc. SC'04, Pittsburgh, PA, November, 2004 In 2005, as part of a summer project initiated by Scott A. Brandt and led by Carlos Maltzahn, Sage Weil created a fully functional file system prototype which adopted the name Ceph. Ceph made its debut with Sage Weil giving two presentations in November 2006, one at USENIX OSDI 2006"Ceph: A scalable, high-performance distributed file system," SA Weil, SA Brandt, EL Miller, DDE Long, C Maltzahn, Proc. OSDI, Seattle, WA, November, 2006 and another at SC'06."CRUSH: Controlled, scalable, decentralized placement of replicated data," SA Weil, SA Brandt, EL Miller, DDE Long, C Maltzahn, SC'06, Tampa, FL, November, 2006 After his graduation in autumn 2007, Weil continued to work on Ceph full-time, and the core development team expanded to include Yehuda Sadeh Weinraub and Gregory Farnum. On March 19, 2010, Linus Torvalds merged the Ceph client into Linux kernel version 2.6.34 which was released on May 16, 2010. In 2012, Weil created Inktank Storage for professional services and support for Ceph. In April 2014, Red Hat purchased Inktank, bringing the majority of Ceph development in-house. In October 2015, the Ceph Community Advisory Board was formed to assist the community in driving the direction of open source software-defined storage technology. The charter advisory board includes Ceph community members from global IT organizations that are committed to the Ceph project, including individuals from Canonical, CERN, Cisco, Fujitsu, Intel, Red Hat, SanDisk, and SUSE. === Release history === * docs.ceph {| class="wikitable" |- |+ Release history |- ! scope="col" | Name ! scope="col" | Release ! scope="col" | First release ! scope="col" | End of life ! scope="col" | Milestones |- | Argonaut | | July 3, 2012 | | First major "stable" release |- | Bobtail | | January 1, 2013 | | |- | Cuttlefish | | May 7, 2013 | | ceph-deploy is stable |- | Dumpling | | August 14, 2013 | May 2015 | namespace, region, monitoring REST API |- | Emperor | | November 9, 2013 | May 2014 | multi-datacenter replication for the radosgw |- | Firefly | | May 7, 2014 | April 2016 | erasure coding, cache tiering, primary affinity, key/value OSD backend (experimental), standalone radosgw (experimental) |- | Giant | | October 29, 2014 | April 2015 | |- | Hammer | | April 7, 2015 | August 2017 | |- | Infernalis | | November 6, 2015 | April 2016 | |- | Jewel | | April 21, 2016 | 2018-06-01 | Stable CephFS, experimental RADOS backend named BlueStore |- | Kraken | | January 20, 2017 | 2017-08-01 | BlueStore is stable |- | Luminous | | August 29, 2017 | | |- | Mimic | | June 1, 2018 | | snapshots are stable ,Beast is stable |- | Nautilus | | March 19, 2019 | | |- | Octopus | | March 23, 2020 | | |- | Pacific | | | | |} ==Etymology== The name "Ceph" is an abbreviation of "cephalopod", a class of molluscs that includes the octopus. The name (emphasized by the logo) suggests the highly parallel behavior of an octopus and was chosen to associate the file system with "Sammy", the banana slug mascot of UCSC. Both cephalopods and banana slugs are molluscs. ==See also== * BeeGFS * Distributed file system * Distributed parallel fault-tolerant file systems * Gfarm file system * GlusterFS * IBM General Parallel File System (GPFS) * LizardFS * Lustre * MapR FS * Moose File System * OrangeFS * Parallel Virtual File System * Quantcast File System * RozoFS * XtreemFS * ZFS ==References== ==Further reading== * Official Ceph documentation * * * * ==External links== * * * SUSE Enterprise Storage (Ceph) * UCSC Systems Research Lab * Storage Systems Research Center * Ceph Performance and Optimization, Ceph Day Frankfurt (2014) at Slideshare Category:Distributed file systems supported by the Linux kernel Category:Free software Category:Network file systems Category:Red Hat software Category:Userspace file systems Category:Virtualization-related software for Linux 
